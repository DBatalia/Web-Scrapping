{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-81-0d87e4fcf8a6>, line 152)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-81-0d87e4fcf8a6>\"\u001b[1;36m, line \u001b[1;32m152\u001b[0m\n\u001b[1;33m    finally:\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import Dependecies \n",
    "from bs4 import BeautifulSoup \n",
    "from splinter import Browser\n",
    "import pandas as pd \n",
    "import requests \n",
    "\n",
    "# Initialize browser\n",
    "def init_browser(): \n",
    "    # Replace the path with your actual path to the chromedriver\n",
    "\n",
    "    #Windows Users\n",
    "    # executable_path = {'executable_path': '/Users/cantu/Desktop/Mission-to-Mars'}\n",
    "    # return Browser('chrome', **executable_path, headless=False)\n",
    "    exec_path = {'executable_path': '/app/.chromedriver/bin/chromedriver'}\n",
    "    return Browser('chrome', headless=True, **exec_path)\n",
    "\n",
    "# Create Mission to Mars global dictionary that can be imported into Mongo\n",
    "mars_info = {}\n",
    "\n",
    "# NASA MARS NEWS\n",
    "def scrape_mars_news():\n",
    "    try: \n",
    "\n",
    "        # Initialize browser \n",
    "        browser = init_browser()\n",
    "\n",
    "        #browser.is_element_present_by_css(\"div.content_title\", wait_time=1)\n",
    "\n",
    "        # Visit Nasa news url through splinter module\n",
    "        url = 'https://mars.nasa.gov/news/'\n",
    "        browser.visit(url)\n",
    "\n",
    "        # HTML Object\n",
    "        html = browser.html\n",
    "\n",
    "        # Parse HTML with Beautiful Soup\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "        # Retrieve the latest element that contains news title and news_paragraph\n",
    "        news_title = soup.find('div', class_='content_title').find('a').text\n",
    "        news_paragraph = soup.find('div', class_='article_teaser_body').text\n",
    "\n",
    "        # Dictionary entry from MARS NEWS\n",
    "        mars_info['news_title'] = news_title\n",
    "        mars_info['news_paragraph'] = news_p\n",
    "\n",
    "        return mars_info\n",
    "\n",
    "    finally:\n",
    "        browser.quit()\n",
    "        \n",
    "# FEATURED IMAGE\n",
    "def scrape_mars_image():\n",
    "\n",
    "    try: \n",
    "\n",
    "        # Initialize browser \n",
    "        browser = init_browser()\n",
    "\n",
    "        # Visit Mars Space Images through splinter module\n",
    "        image_url_featured = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "        browser.visit(image_url_featured)# Visit Mars Space Images through splinter module\n",
    "\n",
    "        # HTML Object \n",
    "        html_image = browser.html\n",
    "\n",
    "        # Parse HTML with Beautiful Soup\n",
    "        soup = BeautifulSoup(html_image, 'html.parser')\n",
    "\n",
    "        # Retrieve background-image url from style tag \n",
    "        featured_image_url  = soup.find('article')['style'].replace('background-image: url(','').replace(');', '')[1:-1]\n",
    "\n",
    "        # Website Url \n",
    "        main_url = 'https://www.jpl.nasa.gov'\n",
    "\n",
    "        # Concatenate website url with scrapped route\n",
    "        featured_image_url = main_url + featured_image_url\n",
    "\n",
    "        # Display full link to featured image\n",
    "        featured_image_url \n",
    "\n",
    "        # Dictionary entry from FEATURED IMAGE\n",
    "        mars_info['featured_image_url'] = featured_image_url \n",
    "        \n",
    "        return mars_info\n",
    "    finally:\n",
    "\n",
    "        browser.quit()\n",
    "\n",
    "# Mars Weather \n",
    "def scrape_mars_weather():\n",
    "\n",
    "    try: \n",
    "\n",
    "        # Initialize browser \n",
    "        browser = init_browser()\n",
    "\n",
    "        #browser.is_element_present_by_css(\"div\", wait_time=1)\n",
    "\n",
    "        # Visit Mars Weather Twitter through splinter module\n",
    "        weather_url = 'https://twitter.com/marswxreport?lang=en'\n",
    "        browser.visit(weather_url)\n",
    "\n",
    "        # HTML Object \n",
    "        html_weather = browser.html\n",
    "\n",
    "        # Parse HTML with Beautiful Soup\n",
    "        soup = BeautifulSoup(html_weather, 'html.parser')\n",
    "\n",
    "        # Find all elements that contain tweets\n",
    "        tweets=soup.find(\"ol\",class_=\"stream-items\")\n",
    "#Find the src for the sloth image\n",
    "        mars_weather = tweets.find('p', class_=\"tweet-text\").text\n",
    "\n",
    "        # Dictionary entry from WEATHER TWEET\n",
    "        mars_info['mars_weather'] = mars_weather\n",
    "        \n",
    "        return mars_info\n",
    "    finally:\n",
    "        browser.quit()\n",
    "\n",
    "\n",
    "# Mars Facts\n",
    "def scrape_mars_facts():\n",
    "\n",
    "# Visit Mars facts url \n",
    "    facts_url = 'http://space-facts.com/mars/'\n",
    "\n",
    "# Use Panda's `read_html` to parse the url\n",
    "    mars_facts = pd.read_html(facts_url)\n",
    "\n",
    "# Find the mars facts DataFrame in the list of DataFrames as assign it to `mars_df`\n",
    "    mars_df = mars_facts[0]\n",
    "\n",
    "# Assign the columns `['Description', 'Value']`\n",
    "    mars_df.columns = ['Description','Value']\n",
    "\n",
    "# Set the index to the `Description` column without row indexing\n",
    "    mars_df.set_index('Description', inplace=True)\n",
    "\n",
    "# Save html code to folder Assets\n",
    "    data = mars_df.to_html()\n",
    "\n",
    "# Dictionary entry from MARS FACTS\n",
    "    mars_info['mars_facts'] = data\n",
    "\n",
    "    return mars_info\n",
    "    finally:\n",
    "    browser.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://twitter.com/marswxreport?lang=en\"\n",
    "soup =scrape_info(url)\n",
    "tweets=soup.find(\"ol\",class_=\"stream-items\")\n",
    "#Find the src for the sloth image\n",
    "mars_weather = tweets.find('p', class_=\"tweet-text\").text\n",
    "\n",
    "print(mars_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visit the Mars Facts webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_browser():\n",
    "    executable_path = {'executable_path': 'C:/chromedriver/chromedriver.exe'}\n",
    "    return Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General browser scraping\n",
    "def scrape_info(url):\n",
    "    with init_browser() as browser:\n",
    "    #browser = init_browser()\n",
    "    #url=\"paste url path\"\n",
    "        browser.visit(url)\n",
    "        time.sleep(1)\n",
    "        html = browser.html\n",
    "    return bs(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_fact_url=\"https://space-facts.com/mars/\"\n",
    "mars_fact_soup =scrape_info(mars_fact_url)\n",
    "fact_table = mars_fact_soup.find('table', class_=\"tablepress tablepress-id-mars\")\n",
    "print(fact_table.prettify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column1 = fact_table.find_all('td', class_='column-1')\n",
    "column2 = fact_table.find_all('td', class_='column-2')\n",
    "\n",
    "facets = []\n",
    "values = []\n",
    "\n",
    "for row in column1:\n",
    "    facet = row.text.strip()\n",
    "    facets.append(facet)\n",
    "    \n",
    "for row in column2:\n",
    "    value = row.text.strip()\n",
    "    values.append(value)\n",
    "    \n",
    "mars_facts = pd.DataFrame({\n",
    "    \"Facet\":facets,\n",
    "    \"Value\":values\n",
    "    })\n",
    "\n",
    "mars_facts_html = mars_facts.to_html(header=False, index=False)\n",
    "mars_facts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_browser():\n",
    "    executable_path = {'executable_path': 'C:/chromedriver/chromedriver.exe'}\n",
    "    return Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General browser scraping\n",
    "def scrape_info(url):\n",
    "    with init_browser() as browser:\n",
    "    #browser = init_browser()\n",
    "    #url=\"paste url path\"\n",
    "        browser.visit(url)\n",
    "        time.sleep(1)\n",
    "        html = browser.html\n",
    "    return bs(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
